{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tiktoken\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from modeling_gpt2 import GPT2\n",
    "import requests\n",
    "from generate import text_to_token_ids ,  token_ids_to_text , generate \n",
    "from utils import create_dataloader_v1 , calc_loss_batch , calc_loss_loader , evaluate_model\n",
    "\n",
    "# Configuration for GPT-2 model\n",
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_len\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"num_heads\": 12,\n",
    "    \"num_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}\n",
    "\n",
    "# Training Hyperparameters\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 3e-4\n",
    "BATCH_SIZE = 16\n",
    "EVAL_INTERVAL = 74  # Evaluate every 100 steps\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "START_CONTEXT = \"First Person:\"\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Load the GPT-2 model\n",
    "model = GPT2(GPT_CONFIG).to(DEVICE)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "# Function to load text data\n",
    "def load_text_file(path_to_text=None, url=None):\n",
    "    if path_to_text:\n",
    "        with open(path_to_text, 'r', encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    elif url:\n",
    "        response = requests.get(url)\n",
    "        return response.text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Load dataset\n",
    "text = load_text_file(path_to_text=\"/teamspace/studios/this_studio/GPT2/model/shakespeare.txt\")\n",
    "split_idx = int(0.90 * len(text))\n",
    "train_data, val_data = text[:split_idx], text[split_idx:]\n",
    "\n",
    "print(\"Train data:\",len(train_data))\n",
    "print(\"Val data:\",len(val_data))\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_loader = create_dataloader_v1(\n",
    "    txt=train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_length=GPT_CONFIG['context_len'],\n",
    "    stride=GPT_CONFIG['context_len'],\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    txt=val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_length=GPT_CONFIG['context_len'],\n",
    "    stride=GPT_CONFIG['context_len'],\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# Initialize optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "\n",
    "# Lists to track metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "grad_norms = []\n",
    "step_times = []\n",
    "learning_rates = []\n",
    "\n",
    "#  torch.compile makes PyTorch code run faster by JIT-compiling PyTorch code into optimized kernels, all while requiring minimal code changes.\n",
    "model = torch.compile(model)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Training loop\n",
    "print(\"STARTING TO TRAIN\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for step, (x, y) in progress_bar:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Use autocast with float16 for mixed precision training\n",
    "        with torch.autocast(device_type=DEVICE , dtype=torch.float16):\n",
    "            loss = calc_loss_batch(x, y, model, DEVICE)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        norm = nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        train_losses.append(loss.item())\n",
    "        grad_norms.append(norm)\n",
    "        end_time = time.time()\n",
    "        step_times.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
    "        learning_rates.append(scheduler.get_last_lr()[0])\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'grad_norm': f\"{norm:.4f}\",\n",
    "            'step_time': f\"{step_times[-1]:.4f}ms\",  # Display time in milliseconds\n",
    "            'lr': f\"{learning_rates[-1]:.6f}\"\n",
    "        })\n",
    "        \n",
    "        # Generate sample text after each step\n",
    "        token_ids = generate(\n",
    "                    model=model,\n",
    "                    device=DEVICE,\n",
    "                    idx=text_to_token_ids(START_CONTEXT, tokenizer),\n",
    "                    max_new_tokens=10,\n",
    "                    context_len=GPT_CONFIG[\"context_len\"],\n",
    "        )\n",
    "        \n",
    "        if (step + 1) % EVAL_INTERVAL == 0:\n",
    "            train_loss, val_loss = evaluate_model(model, train_loader, val_loader, DEVICE)\n",
    "            \n",
    "            print(f\"\\nStep {step+1} - Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "            print(f\"Sample text:\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "            \n",
    "    scheduler.step()\n",
    "\n",
    "    # End of epoch evaluation\n",
    "    train_loss, val_loss = evaluate_model(model, train_loader, val_loader, DEVICE)\n",
    "    print(f\"\\nEpoch {epoch+1} - Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"model.pth\",\n",
    "    path_in_repo=\"gpt_model.pt\",\n",
    "    repo_id=\"damerajee/smallgpt\",\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "First Citizen:\n",
    "I say unto you, what \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate(\n",
    "                    model=model,\n",
    "                    device=DEVICE,\n",
    "                    idx=text_to_token_ids(START_CONTEXT, tokenizer),\n",
    "                    max_new_tokens=10,\n",
    "                    context_len=GPT_CONFIG[\"context_len\"],\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sample text:\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
